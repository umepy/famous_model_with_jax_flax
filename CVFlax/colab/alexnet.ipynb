{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/umepy/famous_model_with_jax_flax.git\n",
    "%cd famous_model_with_jax_flax\n",
    "!git checkout -b alexnet origin/alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q pip optax\n",
    "!pip install -U -q git+https://github.com/google/flax.git\n",
    "!pip install -U torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path(os.path.abspath(\"__file__\")).parent.parent.parent))\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.tools.colab_tpu\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from CVFlax.models import AlexNet\n",
    "from CVFlax.utils.preprocess import alexnet_dataloader, download_food101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seems already splitted, so skipping to split dataset\n"
     ]
    }
   ],
   "source": [
    "download_food101()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(key, learning_rate):\n",
    "    model = AlexNet(output_dim=101)\n",
    "    params = model.init(key, jnp.ones([1,227,227,3]))['params']\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(logits, y):\n",
    "  accuracy = jnp.mean(jnp.argmax(logits, -1) == y)\n",
    "  return accuracy\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, x, y):\n",
    "  def loss_fn(params):\n",
    "    logits = AlexNet(output_dim=101).apply({'params':params}, x)\n",
    "    one_hot_labels = jax.nn.one_hot(y, num_classes=101)\n",
    "    loss = -jnp.mean(jnp.sum(one_hot_labels * logits, axis=-1))\n",
    "    return loss, logits\n",
    "  \n",
    "  (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  metrics = {\n",
    "      'loss': loss,\n",
    "      'accuracy': compute_accuracy(logits, y),\n",
    "  }\n",
    "  return state, metrics\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(state, x, y):\n",
    "  logits = AlexNet().apply({'params':state.params}, x)\n",
    "  return compute_accuracy(logits, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(state, dataloader, epoch):\n",
    "  batch_metrics = []\n",
    "  track_metrics = {'loss':[], 'accuracy':[]}\n",
    "  with tqdm(total=len(dataloader)) as tq:\n",
    "    for cnt, (x, y) in enumerate(dataloader):\n",
    "      tq.update(1)\n",
    "      state, metrics = train_step(state, x, y) # update state \n",
    "      batch_metrics.append(metrics)\n",
    "      print(metrics)\n",
    "      track_metrics['loss'].append(metrics['loss'])\n",
    "      track_metrics['accuracy'].append(metrics['accuracy'])\n",
    "\n",
    "      if cnt%20==0:\n",
    "        print(f'Epoch: {cnt}\\tloss: {np.mean(track_metrics[\"loss\"])}\\taccuracy: {np.mean(track_metrics[\"accuracy\"])}')\n",
    "        track_metrics = {'loss':[], 'accuracy':[]}\n",
    "  \n",
    "  batch_metrics_np = jax.device_get(batch_metrics)\n",
    "  epoch_metrics_np = {\n",
    "      k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "      for k in batch_metrics_np[0]\n",
    "  }\n",
    "  return state, epoch_metrics_np\n",
    "\n",
    "\n",
    "def evaluate_model(state, x, y):\n",
    "  metrics = eval_step(state, x, y)\n",
    "  metrics = jax.device_get(metrics)\n",
    "  metrics = jax.tree_map(lambda x: x.item(), metrics)  # np.ndarray -> scalar\n",
    "  return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'output_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m128\u001b[39m\n\u001b[1;32m      4\u001b[0m key \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mPRNGKey(\u001b[39m0\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m state \u001b[39m=\u001b[39m create_train_state(key, learning_rate)\n\u001b[1;32m      6\u001b[0m train_loader, test_loader \u001b[39m=\u001b[39m alexnet_dataloader(batch_size)\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n",
      "\u001b[1;32m/workspaces/famous_model_with_jax_flax/CVFlax/jupyter/alexnet.ipynb Cell 5'\u001b[0m in \u001b[0;36mcreate_train_state\u001b[0;34m(key, learning_rate)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B623a5c70726f6a656374735c66616d6f75735f6d6f64656c5f776974685f6a61785f666c6178/workspaces/famous_model_with_jax_flax/CVFlax/jupyter/alexnet.ipynb#ch0000007vscode-remote?line=2'>3</a>\u001b[0m params \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39minit(key, jnp\u001b[39m.\u001b[39mones([\u001b[39m1\u001b[39m,\u001b[39m227\u001b[39m,\u001b[39m227\u001b[39m,\u001b[39m3\u001b[39m]))[\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B623a5c70726f6a656374735c66616d6f75735f6d6f64656c5f776974685f6a61785f666c6178/workspaces/famous_model_with_jax_flax/CVFlax/jupyter/alexnet.ipynb#ch0000007vscode-remote?line=3'>4</a>\u001b[0m tx \u001b[39m=\u001b[39m optax\u001b[39m.\u001b[39madam(learning_rate)\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B623a5c70726f6a656374735c66616d6f75735f6d6f64656c5f776974685f6a61785f666c6178/workspaces/famous_model_with_jax_flax/CVFlax/jupyter/alexnet.ipynb#ch0000007vscode-remote?line=4'>5</a>\u001b[0m \u001b[39mreturn\u001b[39;00m train_state\u001b[39m.\u001b[39;49mTrainState\u001b[39m.\u001b[39;49mcreate(apply_fn\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mapply, params\u001b[39m=\u001b[39;49mparams, tx\u001b[39m=\u001b[39;49mtx, output_dim\u001b[39m=\u001b[39;49m\u001b[39m101\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/flax/training/train_state.py:87\u001b[0m, in \u001b[0;36mTrainState.create\u001b[0;34m(cls, apply_fn, params, tx, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/dist-packages/flax/training/train_state.py?line=84'>85</a>\u001b[0m \u001b[39m\"\"\"Creates a new instance with `step=0` and initialized `opt_state`.\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/dist-packages/flax/training/train_state.py?line=85'>86</a>\u001b[0m opt_state \u001b[39m=\u001b[39m tx\u001b[39m.\u001b[39minit(params)\n\u001b[0;32m---> <a href='file:///usr/local/lib/python3.9/dist-packages/flax/training/train_state.py?line=86'>87</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/dist-packages/flax/training/train_state.py?line=87'>88</a>\u001b[0m     step\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/dist-packages/flax/training/train_state.py?line=88'>89</a>\u001b[0m     apply_fn\u001b[39m=\u001b[39;49mapply_fn,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/dist-packages/flax/training/train_state.py?line=89'>90</a>\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/dist-packages/flax/training/train_state.py?line=90'>91</a>\u001b[0m     tx\u001b[39m=\u001b[39;49mtx,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/dist-packages/flax/training/train_state.py?line=91'>92</a>\u001b[0m     opt_state\u001b[39m=\u001b[39;49mopt_state,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/dist-packages/flax/training/train_state.py?line=92'>93</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/dist-packages/flax/training/train_state.py?line=93'>94</a>\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'output_dim'"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "key = jax.random.PRNGKey(0)\n",
    "state = create_train_state(key, learning_rate)\n",
    "train_loader, test_loader = alexnet_dataloader(batch_size)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "  state, train_metrics = train_epoch(state, train_loader, epoch)\n",
    "  print(f\"Train epoch: {epoch}, loss: {train_metrics['loss']:.4}, accuracy: {train_metrics['accuracy'] * 100:.4}\")\n",
    "\n",
    "\n",
    "  #test_metrics = eval_step(state, test_images, test_lbls)\n",
    "  #print(f\"Test epoch: {epoch}, accuracy: {test_metrics * 100:.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
